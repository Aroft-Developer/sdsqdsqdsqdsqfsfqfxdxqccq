// âœ… main.js
import express from "express";
import axios from "axios";
import cors from "cors";
import fs from "fs";
import Groq from "groq-sdk";

const app = express();
app.use(cors({ origin: "https://project-virid-alpha.vercel.app" }));
app.use(express.json());

// âœ… Ping toutes les 5 minutes pour Render
setInterval(() => {
  fetch("https://project-cwgk.onrender.com")
    .then(() => console.log("âœ… Ping sent to keep alive"))
    .catch(() => console.log("âŒ Ping failed"));
}, 5 * 60 * 1000);

// âœ… Chargement des Ã©tablissements
const fullData = JSON.parse(fs.readFileSync("./resultats_ime.json", "utf-8"));
const etablissements = fullData.map(e => ({
  id: String(e.id),
  nom: e.nom || "Nom inconnu",
  type: e.type || "Type inconnu",
  age_min: e.age_min || 0,
  age_max: e.age_max || 21,
  ville: e.ville || "Ville inconnue",
  site_web: e.url_source || "",
  google_maps: e.google_maps || ""
}));

const groq = new Groq({ apiKey: process.env.GROQ_API_KEY });

async function analyserParMorceaux(situation, etabs) {
  const CHUNK_SIZE = 40;
  const resultats = [];
  let justificationGlobal = "";

  const chunks = [];
  for (let i = 0; i < etabs.length; i += CHUNK_SIZE) {
    chunks.push(etabs.slice(i, i + CHUNK_SIZE));
  }

  for (const chunk of chunks) {
    const prompt = `
Tu es un assistant Ã©ducatif spÃ©cialisÃ©.

Voici une situation : "${situation}"

Voici une liste de ${chunk.length} Ã©tablissements :
${JSON.stringify(chunk, null, 2)}

Analyse et sÃ©lectionne au maximum 6 Ã©tablissements pertinents en fonction de la situation (Ã¢ge, profil, besoin, etc.).

Si aucun ne correspond, rÃ©ponds :
{"justification": "Aucun Ã©tablissement pertinent dans ce groupe."}

Sinon, rÃ©ponds :
{
  "resultats": [ ... ],
  "justification": "Pourquoi ces Ã©tablissements sont les meilleurs dans ce groupe."
}

Ne retourne que le JSON. Aucun texte avant ou aprÃ¨s.`;

    try {
      const completion = await groq.chat.completions.create({
        model: "llama3-70b-8192",
        messages: [{ role: "user", content: prompt }],
        temperature: 0.7,
        max_tokens: 1100,
      });

      const raw = completion.choices[0].message.content.trim();
      const match = raw.match(/\{[\s\S]*\}/);
      if (!match) continue;

      const json = JSON.parse(match[0]);

      if (json.resultats && Array.isArray(json.resultats)) {
        resultats.push(...json.resultats);
      }
      if (json.justification) {
        justificationGlobal += "\n" + json.justification;
      }
    } catch (e) {
      console.error("âŒ Erreur dans un chunk :", e.message);
    }
  }

  return {
    resultats: resultats.slice(0, 6),
    justification: justificationGlobal.trim() || "Analyse effectuÃ©e par morceaux.",
  };
}

app.post("/analyse", async (req, res) => {
  try {
    const userRequest = req.body.text;
    if (!userRequest) return res.status(400).json({ error: "texte manquante" });

    const resultatFinal = await analyserParMorceaux(userRequest, etablissements);
    res.json(resultatFinal);

  } catch (err) {
    console.error("âŒ Erreur serveur (/analyse) :", err);
    res.status(500).json({ error: "Erreur serveur" });
  }
});

app.post("/conseil", async (req, res) => {
  try {
    const situation = req.body.text;
    if (!situation) return res.status(400).json({ error: "situation manquante" });

    const prompt = `Tu es un Ã©ducateur spÃ©cialisÃ© expÃ©rimentÃ© qui Ã©change avec un collÃ¨gue Ã©ducateur spÃ©cialisÃ©. 
Dans le cadre de ton mÃ©tier, analyse la situation suivante : "${situation}".
Fournis un conseil professionnel, clair, structurÃ© et orientÃ© solution, destinÃ© Ã  un Ã©ducateur spÃ©cialisÃ©.
Le conseil doit comporter entre 10 et 20 lignes, Ãªtre pragmatique, Ã©viter les gÃ©nÃ©ralitÃ©s, et inclure des pistes d'intervention concrÃ¨tes, ainsi que des points d'attention spÃ©cifiques Ã  cette situation. 
Tu peux Ã©voquer les dÃ©marches Ã  envisager, les acteurs Ã  mobiliser, et les risques Ã  surveiller, toujours dans une optique de soutien efficace au jeune.`;

    const completion = await axios.post(
      "https://api.groq.com/openai/v1/chat/completions",
      {
        model: "llama3-70b-8192",
        messages: [{ role: "user", content: prompt }],
        temperature: 0.7,
        max_tokens: 700,
      },
      {
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${process.env.GROQ_API_KEY}`, // ðŸ” Place ta clÃ© Groq ici
        },
      }
    );

    const responseText = completion.data.choices[0].message.content.trim();
    res.json({ reponse: responseText });

  } catch (err) {
    console.error("âŒ Erreur serveur (conseil) :", err);
    res.status(500).json({ error: "Erreur serveur" });
  }
});

// âœ… Port dynamique pour Render
const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`ðŸš€ Serveur Express lancÃ© sur le port ${PORT}`);
});
